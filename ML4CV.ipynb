{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-11-14T19:58:28.389133Z",
     "iopub.execute_input": "2022-11-14T19:58:28.389528Z",
     "iopub.status.idle": "2022-11-14T19:58:28.418647Z",
     "shell.execute_reply.started": "2022-11-14T19:58:28.389393Z",
     "shell.execute_reply": "2022-11-14T19:58:28.417380Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Things to include in the notebook\n",
    "- Difference between training on the same dataset(s) and crossing dataset\n",
    "- Try to have it random if it should train on an image from MNIST or SVHN. \n",
    "- Try training with different types of preprocessing and data augmentation\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# About the project\n",
    "\n",
    "*This project will seek to discover if machine learning models can train on two different datasets\n",
    "and still deliver acceptable results. The two datasets in question are MNIST and SVHN,\n",
    "both datasets containing numbers. The MNIST dataset contains handwritten digits of a single color,\n",
    "whereas SVHN contains house numbers in different shapes and colors. We will use a siamese model to see\n",
    "if the model can learn features despite its differences in numerical shapes. In addition, we will\n",
    " use a model with two identical networks in parallel, without weight sharing, where each network will\n",
    " have it's own dataset of focus.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Startphase\n",
    "*Before we set up the models and the training we must familiarize with the datasets used.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the MNIST dataset\n",
    "*The MNIST dataset is available through the torchvision package*"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mnist_train_dataset = torchvision.datasets.MNIST(root=\"data\", download=True, transform=ToTensor(), train=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-14T19:58:28.726740Z",
     "iopub.execute_input": "2022-11-14T19:58:28.727033Z",
     "iopub.status.idle": "2022-11-14T19:58:33.509707Z",
     "shell.execute_reply.started": "2022-11-14T19:58:28.727011Z",
     "shell.execute_reply": "2022-11-14T19:58:33.508512Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the SVHN dataset\n",
    "*The SVHN dataset is also available through torchvision. However, it has a different logic for splitting into train and test*"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "svhn_train_dataset = torchvision.datasets.SVHN(root=\"data\", split='extra', download=True, transform=ToTensor())"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-14T19:58:33.511835Z",
     "iopub.execute_input": "2022-11-14T19:58:33.512498Z",
     "iopub.status.idle": "2022-11-14T19:59:11.826606Z",
     "shell.execute_reply.started": "2022-11-14T19:58:33.512465Z",
     "shell.execute_reply": "2022-11-14T19:59:11.825040Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/extra_32x32.mat to data/extra_32x32.mat\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1329278602 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3758f8b75071439d8d97ec1c8c58f97e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/hg/fn9xwmwn2cn9dbs9b732cgq00000gn/T/ipykernel_60941/1888684212.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msvhn_train_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorchvision\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSVHN\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"data\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msplit\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'extra'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdownload\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mToTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torchvision/datasets/svhn.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, root, split, transform, target_transform, download)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdownload\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdownload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_integrity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torchvision/datasets/svhn.py\u001B[0m in \u001B[0;36mdownload\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    125\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdownload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m         \u001B[0mmd5\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m         \u001B[0mdownload_url\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmd5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torchvision/datasets/utils.py\u001B[0m in \u001B[0;36mdownload_url\u001B[0;34m(url, root, filename, md5, max_redirect_hops)\u001B[0m\n\u001B[1;32m    139\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Downloading \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0murl\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\" to \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mfpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0m_urlretrieve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0murllib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mURLError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mOSError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# type: ignore[attr-defined]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"https\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torchvision/datasets/utils.py\u001B[0m in \u001B[0;36m_urlretrieve\u001B[0;34m(url, filename, chunk_size)\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0murllib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0murlopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murllib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mRequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m\"User-Agent\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUSER_AGENT\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtotal\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlength\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpbar\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mchunk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0miter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mchunk\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m                         \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torchvision/datasets/utils.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0murllib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0murlopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murllib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mRequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m\"User-Agent\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUSER_AGENT\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtotal\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlength\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpbar\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mchunk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0miter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mchunk\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m                         \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    460\u001B[0m             \u001B[0;31m# Amount is given, implement using readinto\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    461\u001B[0m             \u001B[0mb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbytearray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mamt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 462\u001B[0;31m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadinto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    463\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mmemoryview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtobytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    464\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    504\u001B[0m         \u001B[0;31m# connection, and the user is reading more bytes than will be provided\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    505\u001B[0m         \u001B[0;31m# (for example, reading in 1k chunks)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 506\u001B[0;31m         \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadinto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    507\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mn\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    508\u001B[0m             \u001B[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    702\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    703\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 704\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    705\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    706\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the MNIST dataset\n",
    "*As seen below, the MNIST dataset is pretty simple and contains a fairly unanimous representation*"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = np.random.randint(len(mnist_train_dataset), size=(1,)).item()\n",
    "    img, label = mnist_train_dataset[sample_idx]\n",
    "    ax = figure.add_subplot(rows, cols, i)\n",
    "    ax.set_title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze())\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the SVHN dataset\n",
    "*The SVHN dataset differs from the MNIST dataset in several ways. Some images are nearly impossible to interpret,\n",
    "some contains several digits, and all of them are colored. To bridge the gap between these two datasets we will explore\n",
    "some image processing techniques that may help us."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(svhn_train_dataset), size=(1,)).item()\n",
    "    img, label = svhn_train_dataset[sample_idx]\n",
    "    img = img.permute(1, 2, 0)\n",
    "    ax1 = figure.add_subplot(rows, cols, i)\n",
    "    ax1.set_title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze())\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating 2d-array with all the tensors based on label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = len(mnist_train_dataset.classes)\n",
    "\n",
    "mnist_idx_by_class = [np.where(mnist_train_dataset.targets == i)[0] for i in range(0, num_classes)]\n",
    "svhn_idx_by_class = [np.where(svhn_train_dataset.labels == i)[0] for i in range(0, num_classes)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspecting same label images from the two datasets\n",
    "*Here we can see the difference between the datasets, compared on the same number*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_class = np.random.randint(0, num_classes)\n",
    "\n",
    "mnist_idx = random.choice(mnist_idx_by_class[random_class])\n",
    "svhn_idx = random.choice(svhn_idx_by_class[random_class])\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "mnist_img = mnist_train_dataset.data[mnist_idx]\n",
    "svhn_img = svhn_train_dataset.data[svhn_idx]\n",
    "\n",
    "ax = figure.add_subplot(1, 2, 1)\n",
    "ax.set_title(random_class)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(mnist_img.squeeze(), cmap=\"gray\")\n",
    "ax = figure.add_subplot(1, 2, 2)\n",
    "ax.set_title(random_class)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(svhn_img.transpose(1, 2, 0))\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the distribution of labels from both datasets\n",
    "*As we can see the SVHN contains more images in total compared to the MNIST dataset. The SVHN\n",
    "dataset has a high amount of images containing 1, 2 and 3. This may lead to some bias towards those digits.\n",
    "The MNIST dataset generally has a quite even distribution of the digits and has a total of 60k images on the train split.\n",
    "SVHN has a total of 73k images on train, but has an additional 531k images which can be used as\n",
    "extra training data if needed.*\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = np.arange(0, 10)\n",
    "len_mnist = [len(x) for x in mnist_idx_by_class]\n",
    "len_svhn = [len(x) for x in svhn_idx_by_class]\n",
    "\n",
    "width = 0.3  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(labels - width / 2, len_mnist, width, label='MNIST')\n",
    "rects2 = ax.bar(labels + width / 2, len_svhn, width, label='SVHN')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Amount')\n",
    "ax.set_title('Amount of images by label')\n",
    "ax.set_xticks(labels, labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preprocessing - Exploration phase\n",
    "*As we determined above, there is quite some difference between the datasets. In this section we will\n",
    "try to do some preprocessing to bridge the gap between the two*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grayscale\n",
    "*We will start by applying grayscale as this makes the image go from 3 channel to 1 channel. This is necessary as\n",
    "the MNIST images is already 1 channel.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gray_svhn = T.Grayscale()(torch.from_numpy(svhn_img))\n",
    "\n",
    "gray_svhn2 = gray_svhn\n",
    "\n",
    "gray_svhn = torch.movedim(gray_svhn, 0, 2)\n",
    "\n",
    "gray_svhn2 = gray_svhn2.detach().numpy()\n",
    "gray_svhn2 = cv.normalize(gray_svhn2, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\n",
    "gray_svhn2 = 255 - gray_svhn2\n",
    "\n",
    "figure = plt.figure(figsize=(12, 8))\n",
    "ax = figure.add_subplot(1, 3, 1)\n",
    "plt.axis(\"off\")\n",
    "ax.set_title(\"MNIST in grayscale\")\n",
    "plt.imshow(mnist_img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "ax = figure.add_subplot(1, 3, 2)\n",
    "plt.axis(\"off\")\n",
    "ax.set_title(\"SVHN in grayscale\")\n",
    "plt.imshow(gray_svhn.squeeze(), cmap=\"gray\")\n",
    "\n",
    "ax = figure.add_subplot(1, 3, 3)\n",
    "plt.axis(\"off\")\n",
    "ax.set_title(\"SVHN in grayscale normalized similar to MNIST\")\n",
    "plt.imshow(gray_svhn2.squeeze(), cmap=\"gray\")\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Threshold\n",
    "*Another technique we will try is thresholding the image. By doing this we will get a output image only\n",
    "consisting of black and white. As we can see below, the threshold delivers some questionable results. Although\n",
    "it is able to deliver only black and white, it still adds noise to the image.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svhn = svhn_img.transpose(1, 2, 0)\n",
    "mnist = mnist_img.detach().numpy()\n",
    "\n",
    "svhn_grey = cv.cvtColor(svhn, cv.COLOR_BGR2GRAY)\n",
    "svhn_grey = cv.normalize(svhn_grey, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\n",
    "svhn_grey = svhn_grey.astype(np.uint8)\n",
    "\n",
    "#Should find a good number for reduction on adaptive threshold, e.g. the last number\n",
    "svhn_threshold = cv.adaptiveThreshold(svhn_grey, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 0)\n",
    "#svhn_threshold = cv.bitwise_not(svhn_threshold)\n",
    "\n",
    "#Threshold reduction number needs to be 0 on mnist\n",
    "mnist = cv.normalize(mnist, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
    "mnist = mnist.astype(np.uint8)\n",
    "mnist_threshold = cv.adaptiveThreshold(mnist, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 0)\n",
    "\n",
    "figure = plt.figure(figsize=(12, 12))\n",
    "\n",
    "ax = figure.add_subplot(2, 2, 1)\n",
    "ax.set_title(\"SVHN Original\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(svhn, cmap=\"gray\")\n",
    "\n",
    "ax = figure.add_subplot(2, 2, 2)\n",
    "ax.set_title(\"SVHN Adaptive threshold\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(svhn_threshold, cmap=\"gray\")\n",
    "\n",
    "ax = figure.add_subplot(2, 2, 3)\n",
    "ax.set_title(\"MNIST Original\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(mnist, cmap=\"gray\")\n",
    "\n",
    "ax = figure.add_subplot(2, 2, 4)\n",
    "ax.set_title(\"MNIST Adaptive threshold\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(mnist_threshold, cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Equalize Hist and CLAHE\n",
    "*Last we will try equalize histogram and CLAHE + grayscale. We will first convert the images into grayscale\n",
    "before applying the different techniques, leaving us with a 1 channel image.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = svhn\n",
    "\n",
    "image = cv.normalize(image, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\n",
    "image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "image = image.astype(np.uint8)\n",
    "\n",
    "image2 = image\n",
    "image4 = image\n",
    "\n",
    "image = cv.equalizeHist(image)\n",
    "clahe = cv.createCLAHE(clipLimit=3., )\n",
    "image2 = clahe.apply(image2)\n",
    "\n",
    "image3 = cv.adaptiveThreshold(image, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 2)\n",
    "image4 = cv.adaptiveThreshold(image4, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 20)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = figure.add_subplot(2, 2, 1)\n",
    "ax.set_title(\"EqualizeHist\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "ax = figure.add_subplot(2, 2, 2)\n",
    "ax.set_title(\"CLAHE\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image2.squeeze(), cmap=\"gray\")\n",
    "ax = figure.add_subplot(2, 2, 3)\n",
    "ax.set_title(\"EqualizeHist + Adaptive Threshold\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image3.squeeze(), cmap=\"gray\")\n",
    "ax = figure.add_subplot(2, 2, 4)\n",
    "ax.set_title(\"Adaptive Threshold\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image4.squeeze(), cmap=\"gray\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clearing up variables\n",
    "*Upon ending the exploration we will empty some variables to reduce the load as much as possible*\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del svhn_idx_by_class\n",
    "del svhn_train_dataset\n",
    "del mnist_idx_by_class\n",
    "del mnist_train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the model\n",
    "*Let's start with defining the different models that will be used for the training*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using CUDA if possible"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining the loss function\n",
    "*We will use a contrastive loss function, a loss func\n",
    "## Skriv mer her"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2)\n",
    "            + label\n",
    "            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "\n",
    "        return loss_contrastive"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining siamese network model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16 * 7 * 7, 10)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Forward pass\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # forward pass of input 1\n",
    "        output1 = self.forward_once(input1)\n",
    "        # forward pass of input 2\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up the custom dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, train: bool, mnist=False, svhn=False, mix=False):\n",
    "        self.mnist_dataset = None\n",
    "        self.svhn_dataset = None\n",
    "\n",
    "        if mnist:\n",
    "            self.mnist_dataset = torchvision.datasets.MNIST(\"files\", train=train, download=True,\n",
    "                                                        transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "        if svhn:\n",
    "            if train:\n",
    "                split = \"train\"\n",
    "            else:\n",
    "                split = \"test\"\n",
    "\n",
    "            self.svhn_dataset = torchvision.datasets.SVHN(root=\"data\", split=split, download=True, transform=torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                ResizeGrayscale(),\n",
    "                # EqualizeHist(),\n",
    "                AdaptiveThreshold(),\n",
    "                # CLAHE(),\n",
    "\n",
    "            ]))\n",
    "            #self.svhn_dataset.data = T.Grayscale()(self.svhn_dataset.data)\n",
    "            #self.svhn_dataset.data = torch.from_numpy(self.svhn_dataset.data.transpose(0, 2, 3, 1)) / 255\n",
    "\n",
    "        # used to prepare the labels and images path\n",
    "        self.pairs = make_pairs(mix, self.mnist_dataset, self.svhn_dataset)\n",
    "\n",
    "        self.dataset = [self.mnist_dataset] + [self.svhn_dataset]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img1_dataset, img1_index = self.pairs[index][0]\n",
    "        img2_dataset, img2_index = self.pairs[index][1]\n",
    "        matching = self.pairs[index][2]\n",
    "\n",
    "        return self.dataset[img1_dataset].__getitem__(img1_index)[0], self.dataset[img2_dataset].__getitem__(img2_index)[0], matching\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "class ResizeGrayscale:\n",
    "    def __call__(self, sample):\n",
    "        reshaped = T.Resize((28, 28))(sample)\n",
    "        gray_reshaped = T.Grayscale()(reshaped)\n",
    "        return 1 - gray_reshaped\n",
    "\n",
    "class EqualizeHist:\n",
    "    def __call__(self, sample):\n",
    "        sample = sample.numpy()\n",
    "        sample = cv.normalize(sample, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
    "        sample = sample.astype(np.uint8)\n",
    "        sample = cv.equalizeHist(sample[0])\n",
    "        if len(np.where(sample.flatten() > 200)[0]) > len(np.where(sample.flatten() < 200)[0]):\n",
    "            sample = 255 - sample\n",
    "        sample = sample / 255\n",
    "        return torch.from_numpy(image).unsqueeze(0)\n",
    "\n",
    "class CLAHE:\n",
    "    def __call__(self, sample):\n",
    "        sample = sample.numpy()\n",
    "        sample = cv.normalize(sample, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
    "        sample = sample.astype(np.uint8)\n",
    "        clahe = cv.createCLAHE(clipLimit=3., )\n",
    "        sample = clahe.apply(sample)\n",
    "        if len(np.where(sample.flatten() > 200)[0]) > len(np.where(sample.flatten() < 200)[0]):\n",
    "            sample = 255 - sample\n",
    "        sample = sample / 255\n",
    "        return torch.from_numpy(sample).unsqueeze(0)\n",
    "\n",
    "class AdaptiveThreshold:\n",
    "    def __call__(self, sample):\n",
    "        sample = sample.numpy()\n",
    "        sample = cv.normalize(sample, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
    "        sample = sample.astype(np.uint8)\n",
    "        sample = cv.adaptiveThreshold(sample[0], 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 0)\n",
    "        if len(np.where(sample.flatten() > 200)[0]) > len(np.where(sample.flatten() < 200)[0]):\n",
    "            sample = 255 - sample\n",
    "        sample = torch.from_numpy(sample).unsqueeze(0)\n",
    "        sample = sample / 255\n",
    "        return sample\n",
    "\n",
    "\n",
    "def make_pairs(mix, mnist = None, svhn = None):\n",
    "    pairs = []\n",
    "\n",
    "    num_classes = 10\n",
    "\n",
    "    if mix and mnist and svhn:\n",
    "        return mix_pairs(mnist, num_classes, svhn)\n",
    "    if svhn:\n",
    "        svhn_labels = svhn.labels\n",
    "        svhn_idx = [np.where(svhn_labels == i)[0] for i in range(0, num_classes)]\n",
    "        dataset_pos = 1\n",
    "\n",
    "        for anchor_idx in range(len(svhn_labels)):\n",
    "            label = svhn_labels[anchor_idx]\n",
    "\n",
    "            pos_idx = np.random.choice(svhn_idx[label])\n",
    "\n",
    "            pairs.append([(dataset_pos, anchor_idx), (dataset_pos, pos_idx), 0])\n",
    "\n",
    "            negative_label = np.random.randint(0, num_classes)\n",
    "            while negative_label == label:\n",
    "                negative_label = np.random.randint(0, num_classes)\n",
    "\n",
    "            neg_idx = np.random.choice(svhn_idx[negative_label])\n",
    "\n",
    "            pairs.append([(dataset_pos, anchor_idx), (dataset_pos, neg_idx), 1])\n",
    "    if mnist:\n",
    "        ##TODO start from prev index in pairs\n",
    "        mnist_labels = mnist.targets\n",
    "        mnist_idx = [np.where(mnist_labels == i)[0] for i in range(0, num_classes)]\n",
    "        dataset_pos = 0\n",
    "\n",
    "        for anchor_idx in range(len(mnist_labels)):\n",
    "            label = mnist_labels[anchor_idx]\n",
    "\n",
    "            pos_idx = np.random.choice(mnist_idx[label])\n",
    "\n",
    "            pairs.append([(dataset_pos, anchor_idx), (dataset_pos, pos_idx), 0])\n",
    "\n",
    "            negative_label = np.random.randint(0, num_classes)\n",
    "            while negative_label == label:\n",
    "                negative_label = np.random.randint(0, num_classes)\n",
    "\n",
    "            neg_idx = np.random.choice(mnist_idx[negative_label])\n",
    "\n",
    "            pairs.append([(dataset_pos, anchor_idx), (dataset_pos, neg_idx), 1])\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def add_pairs_mix(dataset_labels, dataset_pos, mnist_idx, svhn_idx):\n",
    "    pairs = []\n",
    "    for anchor_idx in range(len(dataset_labels)):\n",
    "        mnist_label = dataset_labels[anchor_idx]\n",
    "\n",
    "        dataset_choice = np.random.randint(0, 2)\n",
    "        #0 = MNIST, 1 = SVHN\n",
    "\n",
    "        if dataset_choice == 0:\n",
    "            pos_idx = np.random.choice(mnist_idx[mnist_label])\n",
    "        else:\n",
    "            pos_idx = np.random.choice(svhn_idx[mnist_label])\n",
    "\n",
    "        pairs.append([(dataset_pos, anchor_idx), (dataset_choice, pos_idx), 0])\n",
    "\n",
    "        negative_label = np.random.randint(0, num_classes )\n",
    "        while negative_label == mnist_label:\n",
    "            negative_label = np.random.randint(0, num_classes)\n",
    "\n",
    "        dataset_choice = np.random.randint(0, 2)\n",
    "\n",
    "        if dataset_choice == 0:\n",
    "            neg_idx = np.random.choice(mnist_idx[negative_label])\n",
    "        else:\n",
    "            neg_idx = np.random.choice(svhn_idx[negative_label])\n",
    "\n",
    "        pairs.append([(dataset_pos, anchor_idx), (dataset_choice, neg_idx), 1])\n",
    "    return pairs\n",
    "\n",
    "# TODO: Consider cleaning up this into a single for loop instead\n",
    "def mix_pairs(mnist, num_classes, svhn):\n",
    "    pairs = []\n",
    "    ### Add mixing of datasets\n",
    "    mnist_labels = mnist.targets\n",
    "    svhn_labels = svhn.labels\n",
    "    mnist_idx = [np.where(mnist_labels == i)[0] for i in range(0, num_classes)]\n",
    "    svhn_idx = [np.where(svhn_labels == i)[0] for i in range(0, num_classes)]\n",
    "    mnist_dataset_pos = 0\n",
    "    svhn_dataset_pos = 1\n",
    "\n",
    "    pairs = pairs + add_pairs_mix(mnist_labels, mnist_dataset_pos, mnist_idx, svhn_idx)\n",
    "\n",
    "    pairs = pairs + add_pairs_mix(svhn_labels, svhn_dataset_pos, mnist_idx, svhn_idx)\n",
    "\n",
    "    return pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspecting the pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = SiameseDataset(train=True, mnist=True, svhn=True, mix=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prep_for_pyplot(image):\n",
    "    return image[0].numpy().transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "\n",
    "first_item, second_item, similar = train_dataset.__getitem__(120003)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = figure.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"First\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(first_item.squeeze(), cmap=\"gray\")\n",
    "\n",
    "ax = figure.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Second\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(second_item.squeeze(), cmap=\"gray\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_dataset = SiameseDataset(train=True, mnist=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset = SiameseDataset(train=False, mnist=True)\n",
    "#val_dataset.pre_grey()\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "net = SiameseNetwork()\n",
    "contrastive_loss = ContrastiveLoss()\n",
    "adam = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    loss = []\n",
    "\n",
    "    for img1, img2, label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        output1, output2 = model(img1, img2)\n",
    "\n",
    "        loss_contrastive = criterion(output1, output2, label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        loss.append(loss_contrastive.item())\n",
    "\n",
    "    loss = np.array(loss)\n",
    "    return loss.mean() / len(dataloader)\n",
    "\n",
    "\n",
    "def save_model(model, name):\n",
    "    model.eval()\n",
    "    # Input to the model\n",
    "    example1 = torch.randn(1, 1, 28, 28)\n",
    "    example2 = torch.randn(1, 1, 28, 28)\n",
    "    traced_script_module = torch.jit.trace(model.cpu(), (example1, example2))\n",
    "    torch.jit.save(traced_script_module, name)\n",
    "\n",
    "\n",
    "def test(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in dataloader:\n",
    "            output1, output2 = model(img1, img2)\n",
    "\n",
    "            loss_contrastive = criterion(output1, output2, label)\n",
    "            loss.append(loss_contrastive.item())\n",
    "\n",
    "        loss = np.array(loss)\n",
    "    return loss.mean() / len(dataloader)\n",
    "\n",
    "\n",
    "best_model = net\n",
    "rounds_without_improvement = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"--EPOCH {epoch+1}--\")\n",
    "\n",
    "    train_loss = train(model=net, optimizer=adam, criterion=contrastive_loss, dataloader=train_dataloader)\n",
    "    print(f\"Train loss {train_loss}\")\n",
    "\n",
    "    val_loss = test(model=net, criterion=contrastive_loss, dataloader=val_dataloader)\n",
    "    print(f\"Val loss {val_loss}\")\n",
    "\n",
    "    if (val_loss < best_loss):\n",
    "        best_loss = val_loss\n",
    "        best_model = net\n",
    "        rounds_without_improvement = 0\n",
    "    else:\n",
    "        rounds_without_improvement += 1\n",
    "\n",
    "    if (rounds_without_improvement > 3 or epoch == epochs-1):\n",
    "        save_model(model=best_model, name=f\"best_model.pt\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(device)\n",
    "test_dataset = SiameseDataset(train=False, mnist=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "#model = torch.jit.load(\"epoch19_model.pt\").to(device)\n",
    "\n",
    "count = 1\n",
    "for img1, img2, label in test_dataloader:\n",
    " #   output1, output2 = model(img1, img2)\n",
    "\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    ax = figure.add_subplot(1, 2, 1)\n",
    "    ax.set_title(\"Img1\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img1.squeeze(), cmap=\"gray\")\n",
    "    ax = figure.add_subplot(1, 2, 2)\n",
    "    ax.set_title(\"Img2\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img2.squeeze(), cmap=\"gray\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # if label == torch.FloatTensor([[0]]):\n",
    "    #     label = \"Same numbers\"\n",
    "    # else:\n",
    "    #     label = \"Different numbers\"\n",
    "\n",
    "    print(label)\n",
    "    # print(F.pairwise_distance(output1, output2).item())\n",
    "    print()\n",
    "\n",
    "    count += 1\n",
    "    if (count > 10):\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ]
}