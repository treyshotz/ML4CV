{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16 * 7 * 7, 10)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Forward pass\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # forward pass of input 1\n",
    "        output1 = self.forward_once(input1)\n",
    "        # forward pass of input 2\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2)\n",
    "            + label\n",
    "            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "\n",
    "        return loss_contrastive"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def test_pipeline(test_dataset, computing_device):\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "    model = torch.jit.load(\"fold0epoch29.pt\").to(computing_device)\n",
    "    count = 1\n",
    "    for img1, img2, label in test_dataloader:\n",
    "        output1, output2 = model(img1, img2)\n",
    "\n",
    "        figure = plt.figure(figsize=(8, 8))\n",
    "        figure.suptitle(f'Image no.{count}', fontsize=16)\n",
    "\n",
    "        ax = figure.add_subplot(1, 2, 1)\n",
    "        ax.set_title(\"Img1\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img1.squeeze(), cmap=\"gray\")\n",
    "        ax = figure.add_subplot(1, 2, 2)\n",
    "        ax.set_title(\"Img2\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img2.squeeze(), cmap=\"gray\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Image no.{count}\")\n",
    "        if label == torch.FloatTensor([[0]]):\n",
    "            label = \"Same numbers\"\n",
    "        else:\n",
    "            label = \"Different numbers\"\n",
    "\n",
    "        print(f\"Correct label: '{label}'\")\n",
    "        print(F.pairwise_distance(output1, output2).item())\n",
    "        print()\n",
    "\n",
    "        count += 1\n",
    "        if (count > 10):\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from contrastive_loss import ContrastiveLoss\n",
    "from model import SiameseNetwork\n",
    "\n",
    "device = \"\"\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    loss = []\n",
    "\n",
    "    for img1, img2, label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img1, img2 = img1.to(device), img2.to(device)\n",
    "        output1, output2 = model(img1, img2)\n",
    "\n",
    "        loss_contrastive = criterion(output1, output2, label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        loss.append(loss_contrastive.item())\n",
    "\n",
    "\n",
    "    loss = np.array(loss)\n",
    "    return loss.mean() / len(dataloader)\n",
    "\n",
    "\n",
    "def save_model(model, name):\n",
    "    model.eval()\n",
    "    # Input to the model\n",
    "    example1 = torch.randn(1, 1, 28, 28)\n",
    "    example2 = torch.randn(1, 1, 28, 28)\n",
    "    traced_script_module = torch.jit.trace(model.cpu(), (example1, example2))\n",
    "    torch.jit.save(traced_script_module, name)\n",
    "\n",
    "\n",
    "def validate(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in dataloader:\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "\n",
    "            loss_contrastive = criterion(output1, output2, label)\n",
    "            loss.append(loss_contrastive.item())\n",
    "\n",
    "        loss = np.array(loss)\n",
    "    return loss.mean() / len(dataloader)\n",
    "\n",
    "\n",
    "def train_pipeline(epochs, k_fold, batch_size, train_dataset, lr, computing_device):\n",
    "    global device\n",
    "    device = computing_device\n",
    "\n",
    "    global contrastive_loss\n",
    "    for fold, (train_idx, val_idx) in enumerate(k_fold.split(train_dataset)):\n",
    "\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "        val_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_subsampler)\n",
    "\n",
    "        net = SiameseNetwork().to(computing_device)\n",
    "        contrastive_loss = ContrastiveLoss()\n",
    "        adam = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "        rounds_without_improvement = 0\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        print(f\"--FOLD {fold + 1}--\\n\")\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"--EPOCH {epoch + 1}--\")\n",
    "\n",
    "            train_loss = train(model=net, optimizer=adam, criterion=contrastive_loss, dataloader=train_dataloader)\n",
    "            print(f\"Train loss {train_loss}\")\n",
    "\n",
    "            val_loss = validate(model=net, criterion=contrastive_loss, dataloader=val_dataloader)\n",
    "            print(f\"Val loss {val_loss}\")\n",
    "\n",
    "            if (val_loss < best_loss):\n",
    "                best_loss = val_loss\n",
    "                best_model = net\n",
    "                rounds_without_improvement = 0\n",
    "            else:\n",
    "                rounds_without_improvement += 1\n",
    "\n",
    "            if (rounds_without_improvement > 3 or epoch == epochs - 1):\n",
    "                save_model(model=net, name=f\"fold{fold}-epoch{epoch}-transforms{train_dataset.transforms}.pt\")\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "\n",
    "class ResizeGrayscale:\n",
    "    def __call__(self, sample):\n",
    "        reshaped = T.Resize((28, 28))(sample)\n",
    "        gray_reshaped = T.Grayscale()(reshaped)\n",
    "        return 1 - gray_reshaped\n",
    "\n",
    "\n",
    "class EqualizeHist:\n",
    "    def __call__(self, sample):\n",
    "        sample = sample.numpy()\n",
    "        sample = cv.normalize(sample, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
    "        sample = sample.astype(np.uint8)\n",
    "        sample = cv.equalizeHist(sample[0])\n",
    "        if len(np.where(sample.flatten() > 200)[0]) > len(np.where(sample.flatten() < 200)[0]):\n",
    "            sample = 255 - sample\n",
    "        sample = sample / 255\n",
    "        return torch.from_numpy(sample).unsqueeze(0)\n",
    "\n",
    "\n",
    "class CLAHE:\n",
    "    def __call__(self, sample):\n",
    "        sample = sample.numpy()\n",
    "        sample = cv.normalize(sample, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
    "        sample = sample.astype(np.uint8)\n",
    "        clahe = cv.createCLAHE(clipLimit=3., )\n",
    "        sample = clahe.apply(sample)\n",
    "        if len(np.where(sample.flatten() > 200)[0]) > len(np.where(sample.flatten() < 200)[0]):\n",
    "            sample = 255 - sample\n",
    "        sample = sample / 255\n",
    "        return torch.from_numpy(sample).unsqueeze(0)\n",
    "\n",
    "\n",
    "class AdaptiveThreshold:\n",
    "    def __call__(self, sample):\n",
    "        sample = sample.numpy()\n",
    "        sample = cv.normalize(sample, None, alpha=0, beta=255, norm_type=cv.NORM_MINMAX, dtype=cv.CV_8UC1)\n",
    "        sample = sample.astype(np.uint8)\n",
    "        sample = cv.adaptiveThreshold(sample[0], 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 0)\n",
    "        if len(np.where(sample.flatten() > 200)[0]) > len(np.where(sample.flatten() < 200)[0]):\n",
    "            sample = 255 - sample\n",
    "        sample = torch.from_numpy(sample).unsqueeze(0)\n",
    "        sample = sample / 255\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transforms import ResizeGrayscale\n",
    "\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, train: bool, transforms, mnist=False, svhn=False, mix=False, ):\n",
    "        self.mnist_dataset = None\n",
    "        self.svhn_dataset = None\n",
    "        self.transforms = transforms\n",
    "\n",
    "        if mnist:\n",
    "            self.mnist_dataset = torchvision.datasets.MNIST(\"files\", train=train, download=True,\n",
    "                                                            transform=torchvision.transforms.Compose([\n",
    "                                                                                                         torchvision.transforms.ToTensor(),\n",
    "                                                                                                     ] + self.transforms))\n",
    "\n",
    "        if svhn:\n",
    "            if train:\n",
    "                split = \"train\"\n",
    "            else:\n",
    "                split = \"test\"\n",
    "\n",
    "            self.svhn_dataset = torchvision.datasets.SVHN(root=\"data\", split=split, download=True,\n",
    "                                                          transform=torchvision.transforms.Compose([\n",
    "                                                                                                       torchvision.transforms.ToTensor(),\n",
    "                                                                                                       ResizeGrayscale(),\n",
    "                                                                                                   ] + self.transforms))\n",
    "\n",
    "        # used to prepare the labels and images path\n",
    "        self.pairs = make_pairs(mix, self.mnist_dataset, self.svhn_dataset)\n",
    "\n",
    "        self.dataset = [self.mnist_dataset] + [self.svhn_dataset]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img1_dataset, img1_index = self.pairs[index][0]\n",
    "        img2_dataset, img2_index = self.pairs[index][1]\n",
    "        matching = self.pairs[index][2]\n",
    "\n",
    "        return self.dataset[img1_dataset].__getitem__(img1_index)[0], \\\n",
    "               self.dataset[img2_dataset].__getitem__(img2_index)[0], matching\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "\n",
    "def make_pairs(mix, mnist=None, svhn=None):\n",
    "    pairs = []\n",
    "\n",
    "    num_classes = 10\n",
    "\n",
    "    if mix and mnist and svhn:\n",
    "        return mix_pairs(mnist, num_classes, svhn)\n",
    "    if svhn:\n",
    "        svhn_labels = svhn.labels\n",
    "        svhn_idx = [np.where(svhn_labels == i)[0] for i in range(0, num_classes)]\n",
    "        dataset_pos = 1\n",
    "\n",
    "        for anchor_idx in range(len(svhn_labels)):\n",
    "            label = svhn_labels[anchor_idx]\n",
    "\n",
    "            pos_idx = np.random.choice(svhn_idx[label])\n",
    "\n",
    "            pairs.append([(dataset_pos, anchor_idx), (dataset_pos, pos_idx), 0])\n",
    "\n",
    "            negative_label = np.random.randint(0, num_classes)\n",
    "            while negative_label == label:\n",
    "                negative_label = np.random.randint(0, num_classes)\n",
    "\n",
    "            neg_idx = np.random.choice(svhn_idx[negative_label])\n",
    "\n",
    "            pairs.append([(dataset_pos, anchor_idx), (dataset_pos, neg_idx), 1])\n",
    "    if mnist:\n",
    "        mnist_labels = mnist.targets\n",
    "        mnist_idx = [np.where(mnist_labels == i)[0] for i in range(0, num_classes)]\n",
    "        dataset_pos = 0\n",
    "\n",
    "        for anchor_idx in range(len(mnist_labels)):\n",
    "            label = mnist_labels[anchor_idx]\n",
    "\n",
    "            pos_idx = np.random.choice(mnist_idx[label])\n",
    "\n",
    "            pairs.append([(dataset_pos, anchor_idx), (dataset_pos, pos_idx), 0])\n",
    "\n",
    "            negative_label = np.random.randint(0, num_classes)\n",
    "            while negative_label == label:\n",
    "                negative_label = np.random.randint(0, num_classes)\n",
    "\n",
    "            neg_idx = np.random.choice(mnist_idx[negative_label])\n",
    "\n",
    "            pairs.append([(dataset_pos, anchor_idx), (dataset_pos, neg_idx), 1])\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def add_pairs_mix(dataset_labels, dataset_pos, mnist_idx, svhn_idx, num_classes):\n",
    "    pairs = []\n",
    "    for anchor_idx in range(len(dataset_labels)):\n",
    "        mnist_label = dataset_labels[anchor_idx]\n",
    "\n",
    "        dataset_choice = np.random.randint(0, 2)\n",
    "        # 0 = MNIST, 1 = SVHN\n",
    "\n",
    "        if dataset_choice == 0:\n",
    "            pos_idx = np.random.choice(mnist_idx[mnist_label])\n",
    "        else:\n",
    "            pos_idx = np.random.choice(svhn_idx[mnist_label])\n",
    "\n",
    "        pairs.append([(dataset_pos, anchor_idx), (dataset_choice, pos_idx), 0])\n",
    "\n",
    "        negative_label = np.random.randint(0, num_classes)\n",
    "        while negative_label == mnist_label:\n",
    "            negative_label = np.random.randint(0, num_classes)\n",
    "\n",
    "        dataset_choice = np.random.randint(0, 2)\n",
    "\n",
    "        if dataset_choice == 0:\n",
    "            neg_idx = np.random.choice(mnist_idx[negative_label])\n",
    "        else:\n",
    "            neg_idx = np.random.choice(svhn_idx[negative_label])\n",
    "\n",
    "        pairs.append([(dataset_pos, anchor_idx), (dataset_choice, neg_idx), 1])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def mix_pairs(mnist, num_classes, svhn):\n",
    "    pairs = []\n",
    "    ### Add mixing of datasets\n",
    "    mnist_labels = mnist.targets\n",
    "    svhn_labels = svhn.labels\n",
    "    mnist_idx = [np.where(mnist_labels == i)[0] for i in range(0, num_classes)]\n",
    "    svhn_idx = [np.where(svhn_labels == i)[0] for i in range(0, num_classes)]\n",
    "    mnist_dataset_pos = 0\n",
    "    svhn_dataset_pos = 1\n",
    "\n",
    "    pairs = pairs + add_pairs_mix(mnist_labels, mnist_dataset_pos, mnist_idx, svhn_idx, num_classes)\n",
    "\n",
    "    pairs = pairs + add_pairs_mix(svhn_labels, svhn_dataset_pos, mnist_idx, svhn_idx, num_classes)\n",
    "\n",
    "    return pairs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Using downloaded and verified file: data/train_32x32.mat\n",
      "--FOLD 1--\n",
      "\n",
      "--EPOCH 1--\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/hg/fn9xwmwn2cn9dbs9b732cgq00000gn/T/ipykernel_86081/862300101.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     86\u001B[0m )\n\u001B[1;32m     87\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m \u001B[0mfirst_config\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall_pipelines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m second_config = Pipelines(\n",
      "\u001B[0;32m/var/folders/hg/fn9xwmwn2cn9dbs9b732cgq00000gn/T/ipykernel_86081/862300101.py\u001B[0m in \u001B[0;36mall_pipelines\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mall_pipelines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 68\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmnist_svhn_mix_pipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     69\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmnist_svhn_pipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmnist_pipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/hg/fn9xwmwn2cn9dbs9b732cgq00000gn/T/ipykernel_86081/862300101.py\u001B[0m in \u001B[0;36mmnist_svhn_mix_pipeline\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     20\u001B[0m         \u001B[0mk_fold\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKFold\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_splits\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mk_fold_splits\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m         \u001B[0mtrain_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSiameseDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmnist\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msvhn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmix\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransforms\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m         train.train_pipeline(epochs=self.epochs, k_fold=k_fold, batch_size=self.batch_size, train_dataset=train_dataset,\n\u001B[0m\u001B[1;32m     23\u001B[0m                              \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m                              computing_device=self.device)\n",
      "\u001B[0;32m~/Documents/Programmering/ML4CV/train.py\u001B[0m in \u001B[0;36mtrain_pipeline\u001B[0;34m(epochs, k_fold, batch_size, train_dataset, lr, computing_device)\u001B[0m\n\u001B[1;32m     81\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"--EPOCH {epoch + 1}--\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 83\u001B[0;31m             \u001B[0mtrain_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0madam\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontrastive_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataloader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     84\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Train loss {train_loss}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Programmering/ML4CV/train.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, optimizer, criterion, dataloader)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mloss_contrastive\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0mloss_contrastive\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss_contrastive\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    361\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    362\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 363\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    364\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    365\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[0;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m     \u001B[0;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 173\u001B[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[1;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import test\n",
    "import train\n",
    "from dataset import SiameseDataset\n",
    "from transforms import AdaptiveThreshold, EqualizeHist\n",
    "\n",
    "\n",
    "class Pipelines:\n",
    "    def __init__(self, k_fold_splits, batch_size, lr, epochs, transforms, device):\n",
    "        self.k_fold_splits = k_fold_splits\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def mnist_svhn_mix_pipeline(self):\n",
    "        k_fold = KFold(n_splits=self.k_fold_splits)\n",
    "        train_dataset = SiameseDataset(train=True, mnist=True, svhn=True, mix=True, transforms=self.transforms)\n",
    "        train.train_pipeline(epochs=self.epochs, k_fold=k_fold, batch_size=self.batch_size, train_dataset=train_dataset,\n",
    "                             lr=self.lr,\n",
    "                             computing_device=self.device)\n",
    "\n",
    "        del train_dataset\n",
    "\n",
    "        test_dataset = SiameseDataset(train=False, mnist=True, svhn=True, mix=True, transforms=self.transforms)\n",
    "        test.test_pipeline(test_dataset=test_dataset, computing_device=self.device)\n",
    "\n",
    "    def mnist_svhn_pipeline(self):\n",
    "        k_fold = KFold(n_splits=self.k_fold_splits)\n",
    "        train_dataset = SiameseDataset(train=True, mnist=True, svhn=True, mix=False, transforms=self.transforms)\n",
    "        train.train_pipeline(epochs=self.epochs, k_fold=k_fold, batch_size=self.batch_size, train_dataset=train_dataset,\n",
    "                             lr=self.lr,\n",
    "                             computing_device=device)\n",
    "\n",
    "        del train_dataset\n",
    "\n",
    "        test_dataset = SiameseDataset(train=False, mnist=True, svhn=True, mix=False, transforms=self.transforms)\n",
    "        test.test_pipeline(test_dataset=test_dataset, computing_device=device)\n",
    "\n",
    "    def mnist_pipeline(self):\n",
    "        k_fold = KFold(n_splits=self.k_fold_splits)\n",
    "        train_dataset = SiameseDataset(train=True, mnist=True, svhn=False, mix=False, transforms=self.transforms)\n",
    "        train.train_pipeline(epochs=self.epochs, k_fold=k_fold, batch_size=self.batch_size, train_dataset=train_dataset,\n",
    "                             lr=self.lr,\n",
    "                             computing_device=device)\n",
    "\n",
    "        del train_dataset\n",
    "\n",
    "        test_dataset = SiameseDataset(train=False, mnist=True, svhn=False, mix=False, transforms=self.transforms)\n",
    "        test.test_pipeline(test_dataset=test_dataset, computing_device=device)\n",
    "\n",
    "    def svhn_pipeline(self):\n",
    "        k_fold = KFold(n_splits=self.k_fold_splits)\n",
    "        train_dataset = SiameseDataset(train=True, mnist=False, svhn=True, mix=False, transforms=self.transforms)\n",
    "        train.train_pipeline(epochs=self.epochs, k_fold=k_fold, batch_size=self.batch_size, train_dataset=train_dataset,\n",
    "                             lr=self.lr,\n",
    "                             computing_device=device)\n",
    "\n",
    "        del train_dataset\n",
    "\n",
    "        test_dataset = SiameseDataset(train=False, mnist=False, svhn=True, mix=False, transforms=self.transforms)\n",
    "        test.test_pipeline(test_dataset=test_dataset, computing_device=device)\n",
    "\n",
    "    def all_pipelines(self):\n",
    "        self.mnist_svhn_mix_pipeline()\n",
    "        self.mnist_svhn_pipeline()\n",
    "        self.mnist_pipeline()\n",
    "        self.svhn_pipeline()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "first_config = Pipelines(\n",
    "    k_fold_splits=5,\n",
    "    batch_size=4084,\n",
    "    lr=0.001,\n",
    "    epochs=20,\n",
    "    transforms=[\n",
    "        AdaptiveThreshold()\n",
    "    ],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "first_config.all_pipelines()\n",
    "\n",
    "second_config = Pipelines(\n",
    "    k_fold_splits=5,\n",
    "    batch_size=128,\n",
    "    lr=0.001,\n",
    "    epochs=20,\n",
    "    transforms=[\n",
    "        EqualizeHist()\n",
    "    ],\n",
    "    device=device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}